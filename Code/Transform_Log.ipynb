{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6d9b863-97fa-49ea-bf07-1d7353a918fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Label` cannot be resolved. Did you mean one of the following? [` Label`, ` Idle Max`, ` Idle Min`, ` Idle Std`, `Idle Mean`].;\n'Project [ Destination Port#17,  Flow Duration#18,  Total Fwd Packets#19,  Total Backward Packets#20, Total Length of Fwd Packets#21,  Total Length of Bwd Packets#22,  Fwd Packet Length Max#23,  Fwd Packet Length Min#24,  Fwd Packet Length Mean#25,  Fwd Packet Length Std#26, Bwd Packet Length Max#27,  Bwd Packet Length Min#28,  Bwd Packet Length Mean#29,  Bwd Packet Length Std#30, Flow Bytes/s#31,  Flow Packets/s#32,  Flow IAT Mean#33,  Flow IAT Std#34,  Flow IAT Max#35,  Flow IAT Min#36, Fwd IAT Total#37,  Fwd IAT Mean#38,  Fwd IAT Std#39,  Fwd IAT Max#40, ... 56 more fields]\n+- Relation [ Destination Port#17, Flow Duration#18, Total Fwd Packets#19, Total Backward Packets#20,Total Length of Fwd Packets#21, Total Length of Bwd Packets#22, Fwd Packet Length Max#23, Fwd Packet Length Min#24, Fwd Packet Length Mean#25, Fwd Packet Length Std#26,Bwd Packet Length Max#27, Bwd Packet Length Min#28, Bwd Packet Length Mean#29, Bwd Packet Length Std#30,Flow Bytes/s#31, Flow Packets/s#32, Flow IAT Mean#33, Flow IAT Std#34, Flow IAT Max#35, Flow IAT Min#36,Fwd IAT Total#37, Fwd IAT Mean#38, Fwd IAT Std#39, Fwd IAT Max#40,... 55 more fields] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     14\u001b[39m df = spark.read.option(\u001b[33m\"\u001b[39m\u001b[33mheader\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m).csv(\u001b[33m\"\u001b[39m\u001b[33m../Data/raw_logs/Friday-WorkingHours-Afternoon-DDos.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Step 3: Transform the raw CSV into a structured log DataFrame\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# We'll create 5 new columns for log processing:\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# - log_level: INFO for BENIGN, ERROR for attacks\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# - timestamp: unique ID for each row (acts as a timestamp)\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# - time: the flow duration from CSV\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m log_df = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlog_level\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLabel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBENIGN\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mINFO\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43motherwise\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mERROR\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# set log level\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m.withColumn(\n\u001b[32m     28\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     29\u001b[39m     col(\u001b[33m\"\u001b[39m\u001b[33mLabel\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# copy the attack type / label\u001b[39;00m\n\u001b[32m     30\u001b[39m ).withColumn(\n\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mevent\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m     concat_ws(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m,                                  \u001b[38;5;66;03m# combine columns into one string\u001b[39;00m\n\u001b[32m     33\u001b[39m               lit(\u001b[33m\"\u001b[39m\u001b[33mFlow on port\u001b[39m\u001b[33m\"\u001b[39m),                \u001b[38;5;66;03m# static text\u001b[39;00m\n\u001b[32m     34\u001b[39m               col(\u001b[33m\"\u001b[39m\u001b[33mDestination Port\u001b[39m\u001b[33m\"\u001b[39m),           \u001b[38;5;66;03m# port number\u001b[39;00m\n\u001b[32m     35\u001b[39m               lit(\u001b[33m\"\u001b[39m\u001b[33mwith total packets\u001b[39m\u001b[33m\"\u001b[39m),         \u001b[38;5;66;03m# static text\u001b[39;00m\n\u001b[32m     36\u001b[39m               col(\u001b[33m\"\u001b[39m\u001b[33mTotal Fwd Packets\u001b[39m\u001b[33m\"\u001b[39m) + col(\u001b[33m\"\u001b[39m\u001b[33mTotal Backward Packets\u001b[39m\u001b[33m\"\u001b[39m))  \u001b[38;5;66;03m# sum packets\u001b[39;00m\n\u001b[32m     37\u001b[39m ).withColumn(\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     39\u001b[39m     monotonically_increasing_id()  \u001b[38;5;66;03m# generates a unique ID for each row\u001b[39;00m\n\u001b[32m     40\u001b[39m ).withColumn(\n\u001b[32m     41\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     42\u001b[39m     col(\u001b[33m\"\u001b[39m\u001b[33mFlow Duration\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# use flow duration as the time column\u001b[39;00m\n\u001b[32m     43\u001b[39m )\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Step 4: Show the first 5 rows of the structured log DataFrame\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# truncate=False ensures we can see the full event message\u001b[39;00m\n\u001b[32m     47\u001b[39m log_df.select(\u001b[33m\"\u001b[39m\u001b[33mlog_level\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mevent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m\"\u001b[39m).show(\u001b[32m5\u001b[39m, truncate=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chinn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:4789\u001b[39m, in \u001b[36mDataFrame.withColumn\u001b[39m\u001b[34m(self, colName, col)\u001b[39m\n\u001b[32m   4784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[32m   4785\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   4786\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   4787\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m   4788\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4789\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chinn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chinn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    171\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    174\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Label` cannot be resolved. Did you mean one of the following? [` Label`, ` Idle Max`, ` Idle Min`, ` Idle Std`, `Idle Mean`].;\n'Project [ Destination Port#17,  Flow Duration#18,  Total Fwd Packets#19,  Total Backward Packets#20, Total Length of Fwd Packets#21,  Total Length of Bwd Packets#22,  Fwd Packet Length Max#23,  Fwd Packet Length Min#24,  Fwd Packet Length Mean#25,  Fwd Packet Length Std#26, Bwd Packet Length Max#27,  Bwd Packet Length Min#28,  Bwd Packet Length Mean#29,  Bwd Packet Length Std#30, Flow Bytes/s#31,  Flow Packets/s#32,  Flow IAT Mean#33,  Flow IAT Std#34,  Flow IAT Max#35,  Flow IAT Min#36, Fwd IAT Total#37,  Fwd IAT Mean#38,  Fwd IAT Std#39,  Fwd IAT Max#40, ... 56 more fields]\n+- Relation [ Destination Port#17, Flow Duration#18, Total Fwd Packets#19, Total Backward Packets#20,Total Length of Fwd Packets#21, Total Length of Bwd Packets#22, Fwd Packet Length Max#23, Fwd Packet Length Min#24, Fwd Packet Length Mean#25, Fwd Packet Length Std#26,Bwd Packet Length Max#27, Bwd Packet Length Min#28, Bwd Packet Length Mean#29, Bwd Packet Length Std#30,Flow Bytes/s#31, Flow Packets/s#32, Flow IAT Mean#33, Flow IAT Std#34, Flow IAT Max#35, Flow IAT Min#36,Fwd IAT Total#37, Fwd IAT Mean#38, Fwd IAT Std#39, Fwd IAT Max#40,... 55 more fields] csv\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules from PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, concat_ws, lit, monotonically_increasing_id\n",
    "\n",
    "# Step 1: Create a Spark session\n",
    "# This is like starting the Spark engine\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BeginnerFriendlyLogProcessing\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Load your CSV files into a Spark DataFrame\n",
    "# 'header=True' tells Spark the first row has column names\n",
    "df = spark.read.option(\"header\", True).csv(\"../Data/raw_logs/Friday-WorkingHours-Afternoon-DDos.csv\")\n",
    "\n",
    "# Step 3: Transform the raw CSV into a structured log DataFrame\n",
    "# We'll create 5 new columns for log processing:\n",
    "# - log_level: INFO for BENIGN, ERROR for attacks\n",
    "# - error: copy of the original Label column\n",
    "# - event: human-readable message about the flow\n",
    "# - timestamp: unique ID for each row (acts as a timestamp)\n",
    "# - time: the flow duration from CSV\n",
    "\n",
    "log_df = df.withColumn(\n",
    "    \"log_level\",\n",
    "    when(col(\"Label\") == \"BENIGN\", \"INFO\").otherwise(\"ERROR\")  # set log level\n",
    ").withColumn(\n",
    "    \"error\",\n",
    "    col(\"Label\")  # copy the attack type / label\n",
    ").withColumn(\n",
    "    \"event\",\n",
    "    concat_ws(\" \",                                  # combine columns into one string\n",
    "              lit(\"Flow on port\"),                # static text\n",
    "              col(\"Destination Port\"),           # port number\n",
    "              lit(\"with total packets\"),         # static text\n",
    "              col(\"Total Fwd Packets\") + col(\"Total Backward Packets\"))  # sum packets\n",
    ").withColumn(\n",
    "    \"timestamp\",\n",
    "    monotonically_increasing_id()  # generates a unique ID for each row\n",
    ").withColumn(\n",
    "    \"time\",\n",
    "    col(\"Flow Duration\")  # use flow duration as the time column\n",
    ")\n",
    "\n",
    "# Step 4: Show the first 5 rows of the structured log DataFrame\n",
    "# truncate=False ensures we can see the full event message\n",
    "log_df.select(\"log_level\", \"timestamp\", \"error\", \"event\", \"time\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d7e58a0-8d40-41e0-b184-571845727e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------+-------+---------------------------------------------------------------------------------+----+\n",
      "|log_level|timestamp                 |error  |event                                                                            |time|\n",
      "+---------+--------------------------+-------+---------------------------------------------------------------------------------+----+\n",
      "|ERROR    |2023-11-20 08:40:50.664842|WARNING|Service: | ServiceA | Message: | Performance Warnings | ClientIP: | 192.168.1.102|28ms|\n",
      "|ERROR    |2023-11-20 08:40:50.672154|DEBUG  |Service: | ServiceA | Message: | File I/O | ClientIP: | 192.168.1.219            |55ms|\n",
      "|ERROR    |2023-11-20 08:40:50.680263|WARNING|Service: | ServiceA | Message: | Performance Warnings | ClientIP: | 192.168.1.173|44ms|\n",
      "|ERROR    |2023-11-20 08:40:50.688973|ERROR  |Service: | ServiceA | Message: | Critical Errors | ClientIP: | 192.168.1.185     |72ms|\n",
      "|ERROR    |2023-11-20 08:40:50.697002|ERROR  |Service: | ServiceB | Message: | Critical Errors | ClientIP: | 192.168.1.194     |56ms|\n",
      "+---------+--------------------------+-------+---------------------------------------------------------------------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, concat_ws, lit, monotonically_increasing_id\n",
    "\n",
    "# 1️⃣ Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BeginnerFriendlyLogProcessing\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2️⃣ Load new dataset\n",
    "df = spark.read.option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"../Data/raw_logs/logdata.csv\")\n",
    "\n",
    "# 3️⃣ Strip spaces from column names (safe step)\n",
    "df = df.toDF(*[c.strip() for c in df.columns])\n",
    "\n",
    "# 4️⃣ Transform into structured log DataFrame (UPDATED LOGIC)\n",
    "log_df = df.withColumn(\n",
    "    \"log_level\",\n",
    "    when(col(\"LogLevel\") == \"INFO\", \"INFO\").otherwise(\"ERROR\")\n",
    ").withColumn(\n",
    "    \"error\",\n",
    "    col(\"LogLevel\")\n",
    ").withColumn(\n",
    "    \"event\",\n",
    "    concat_ws(\n",
    "        \" | \",\n",
    "        lit(\"Service:\"), col(\"Service\"),\n",
    "        lit(\"Message:\"), col(\"Message\"),\n",
    "        lit(\"ClientIP:\"), col(\"ClientIP\")\n",
    "    )\n",
    ").withColumn(\n",
    "    \"timestamp\",\n",
    "    col(\"Timestamp\")\n",
    ").withColumn(\n",
    "    \"time\",\n",
    "    col(\"TimeTaken\")\n",
    ")\n",
    "\n",
    "# 5️⃣ Show first 5 rows\n",
    "log_df.select(\n",
    "    \"log_level\",\n",
    "    \"timestamp\",\n",
    "    \"error\",\n",
    "    \"event\",\n",
    "    \"time\"\n",
    ").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb936e06-520d-48ff-b32a-f3498e0dfcad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
