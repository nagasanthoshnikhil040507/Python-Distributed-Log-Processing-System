{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b19ad906-072b-476c-a1d9-dbf8a424edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Python Practice/Code/speech.txt\", \"w\") as f:\n",
    "    f.write(\"Good morning everyone\\nThis is a Spark file test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dabd97f-10f2-4a15-b837-a8c44aec7950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Stored in File:\n",
      "['Good morning everyone', 'This is a Spark file test', 'Spark is fast and powerful', 'Spark is used for big data processing', 'Everyone can learn Spark', 'Big data needs distributed processing', 'Spark makes data processing easy', 'Good morning Spark users']\n",
      "\n",
      "Number of Records(Lines): 8\n",
      "\n",
      "Number of Words: 39\n",
      "\n",
      "The Lines that Contains Spark Word are:\n",
      " ['This is a Spark file test', 'Spark is fast and powerful', 'Spark is used for big data processing', 'Everyone can learn Spark', 'Spark makes data processing easy', 'Good morning Spark users']\n",
      "\n",
      "The Text Info Converted to UpperCase: ['GOOD MORNING EVERYONE', 'THIS IS A SPARK FILE TEST', 'SPARK IS FAST AND POWERFUL', 'SPARK IS USED FOR BIG DATA PROCESSING', 'EVERYONE CAN LEARN SPARK', 'BIG DATA NEEDS DISTRIBUTED PROCESSING', 'SPARK MAKES DATA PROCESSING EASY', 'GOOD MORNING SPARK USERS']\n",
      "\n",
      "The Frequency of Each Word in Info(Text): [('good', 2), ('everyone', 2), ('this', 1), ('file', 1), ('fast', 1), ('and', 1), ('powerful', 1), ('used', 1), ('for', 1), ('big', 2), ('learn', 1), ('needs', 1), ('distributed', 1), ('easy', 1), ('morning', 2), ('is', 3), ('a', 1), ('spark', 6), ('test', 1), ('data', 3), ('processing', 3), ('can', 1), ('makes', 1), ('users', 1)]\n",
      "\n",
      "After Removing Stop_words: ['good', 'morning', 'everyone', 'spark', 'file', 'test', 'spark', 'fast', 'powerful', 'spark', 'used', 'big', 'data', 'processing', 'everyone', 'can', 'learn', 'spark', 'big', 'data', 'needs', 'distributed', 'processing', 'spark', 'makes', 'data', 'processing', 'easy', 'good', 'morning', 'spark', 'users']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Spark_DataFrames\").getOrCreate()\n",
    "\n",
    "rdd = spark.sparkContext.textFile(\"speech.txt\")\n",
    "print(\"Information Stored in File:\")\n",
    "print(rdd.collect())\n",
    "#------------------------------------------------------1. Line Count---------------------------------------------------------------------\n",
    "\n",
    "line_count = rdd.count()\n",
    "print(\"\\nNumber of Records(Lines):\",line_count)\n",
    "\n",
    "#------------------------------------------------------2. Word Count---------------------------------------------------------------------\n",
    "\n",
    "word_count = rdd.flatMap(lambda x: x.split()).count()\n",
    "print(\"\\nNumber of Words:\",word_count)\n",
    "\n",
    "#------------------------------------------------------3. Filter Lines Containing \"Spark\"-------------------------------------------------\n",
    "\n",
    "spark_lines = rdd.filter(lambda x: \"Spark\" in x)\n",
    "print(\"\\nThe Lines that Contains Spark Word are:\\n\",spark_lines.collect())\n",
    "\n",
    "#------------------------------------------------------4. Convert Text to Uppercase-------------------------------------------------------\n",
    "\n",
    "upper_text = rdd.map(lambda x: x.upper())\n",
    "print(\"\\nThe Text Info Converted to UpperCase:\",upper_text.collect())\n",
    "\n",
    "#------------------------------------------------------5. Find Most Frequent Word(s)-------------------------------------------------------\n",
    "\n",
    "word_freq = (\n",
    "    rdd.flatMap(lambda x: x.split())\n",
    "       .map(lambda word: (word.lower(), 1))\n",
    "       .reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "print(\"\\nThe Frequency of Each Word in Info(Text):\",word_freq.collect())\n",
    "\n",
    "#------------------------------------------------------6. Remove Stop Words---------------------------------------------------------------\n",
    "\n",
    "stop_words = {\"is\", \"a\", \"for\", \"and\", \"this\", \"to\"}\n",
    "clean_words = (\n",
    "    rdd.flatMap(lambda x: x.split())\n",
    "       .map(lambda word: word.lower())\n",
    "       .filter(lambda word: word not in stop_words)\n",
    ")\n",
    "print(\"\\nAfter Removing Stop_words:\",clean_words.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f6ec8f-a0f9-4140-be6d-e96a024a4fea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
